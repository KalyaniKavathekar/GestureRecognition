{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import cv2 \n",
    "from imageio import imread \n",
    "import imageio.v3 as iio\n",
    "#from skimage.transform  import resize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "import datetime \n",
    "import os\n",
    "import abc\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "import keras as Keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_doc = np.random.permutation(open('../Project_data/train - Copy.csv').readlines())\n",
    "train_doc = np.random.permutation(open('../Project_data/train - Copy.csv').readlines())\n",
    "#print(train_doc)\n",
    "val_doc = np.random.permutation(open('../Project_data/val - Copy.csv').readlines())\n",
    "#batch_size = 32 #16,32,64 #experiment with the batch size\n",
    "project_folder='../Project_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Model Builder class in order to execute gnerator which has 5 functions - \n",
    "#1. Path Settings \n",
    "#2. Set Image Properties \n",
    "#3. Hyperparameter settings \n",
    "#4. Generator functions - Which will return yield \n",
    "#5. Batch data which will get called internally in generator data as code is same for remaining sequences as well\n",
    "\n",
    "class ModelBuilder(metaclass= abc.ABCMeta):\n",
    "    \n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train - Copy.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val - Copy.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "\n",
    "        \n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = iio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=cv2.resize(image,(self.image_height,self.image_width  ))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=cv2.resize(cropped,(self.image_height,self.image_width ))\n",
    "\n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "    \n",
    "    \n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq=\"epoch\")\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Model Building \n",
    "\n",
    "Here we have done several excersize which includes - \n",
    "\n",
    "1. Different experiment on Batch size\n",
    "2. Different experiment on Image size\n",
    "3. Different Layers & Different modules \n",
    "4. Batch Normalization & Dropouts \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Conv3D  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " class ModelConv3D1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 30, 160, 160, 16   1312      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 30, 160, 160, 16   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 30, 160, 160, 16   64        \n",
      " chNormalization)            )                                   \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPoolin  (None, 15, 80, 80, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 15, 80, 80, 32)    4128      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 15, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 15, 80, 80, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPoolin  (None, 7, 40, 40, 32)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 7, 40, 40, 64)     16448     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 7, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 7, 40, 40, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPoolin  (None, 3, 20, 20, 64)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 3, 20, 20, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPoolin  (None, 1, 10, 10, 128)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               1638528   \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1736389 (6.62 MB)\n",
      "Trainable params: 1735525 (6.62 MB)\n",
      "Non-trainable params: 864 (3.38 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=40,num_epochs=1)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "conv_3d1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model - Conv3D \n",
    "#### Image Size - 160 X 160, Batch size - 30, epochs - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1736389\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\KALYANI\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\KALYANI\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5023 - categorical_accuracy: 0.4344\n",
      "Epoch 1: saving model to model_init_2024-02-2708_04_16.006109\\model-00001-1.50233-0.43439-4.15604-0.21000.h5\n",
      "23/23 [==============================] - 62s 3s/step - loss: 1.5023 - categorical_accuracy: 0.4344 - val_loss: 4.1560 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8509 - categorical_accuracy: 0.6923\n",
      "Epoch 2: saving model to model_init_2024-02-2708_04_16.006109\\model-00002-0.85092-0.69231-6.47878-0.22000.h5\n",
      "23/23 [==============================] - 46s 2s/step - loss: 0.8509 - categorical_accuracy: 0.6923 - val_loss: 6.4788 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5268 - categorical_accuracy: 0.8190\n",
      "Epoch 3: saving model to model_init_2024-02-2708_04_16.006109\\model-00003-0.52683-0.81900-5.54399-0.21000.h5\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.5268 - categorical_accuracy: 0.8190 - val_loss: 5.5440 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4279 - categorical_accuracy: 0.8462\n",
      "Epoch 4: saving model to model_init_2024-02-2708_04_16.006109\\model-00004-0.42792-0.84615-5.58795-0.25000.h5\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.4279 - categorical_accuracy: 0.8462 - val_loss: 5.5879 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4681 - categorical_accuracy: 0.8386\n",
      "Epoch 5: saving model to model_init_2024-02-2708_04_16.006109\\model-00005-0.46811-0.83861-5.72439-0.31000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "23/23 [==============================] - 106s 5s/step - loss: 0.4681 - categorical_accuracy: 0.8386 - val_loss: 5.7244 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27f38d04290>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=5)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model - Conv3D \n",
    "#### Image Size - 100 X 100, Batch size - 30, epochs - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 687813\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6273 - categorical_accuracy: 0.4223\n",
      "Epoch 1: saving model to model_init_2024-02-2708_11_07.834561\\model-00001-1.62735-0.42232-3.18315-0.21000.h5\n",
      "23/23 [==============================] - 174s 8s/step - loss: 1.6273 - categorical_accuracy: 0.4223 - val_loss: 3.1831 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0001 - categorical_accuracy: 0.6229\n",
      "Epoch 2: saving model to model_init_2024-02-2708_11_07.834561\\model-00002-1.00005-0.62293-5.57282-0.23000.h5\n",
      "23/23 [==============================] - 159s 7s/step - loss: 1.0001 - categorical_accuracy: 0.6229 - val_loss: 5.5728 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8106 - categorical_accuracy: 0.6772\n",
      "Epoch 3: saving model to model_init_2024-02-2708_11_07.834561\\model-00003-0.81064-0.67722-7.08992-0.17000.h5\n",
      "23/23 [==============================] - 150s 7s/step - loss: 0.8106 - categorical_accuracy: 0.6772 - val_loss: 7.0899 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6913 - categorical_accuracy: 0.7345\n",
      "Epoch 4: saving model to model_init_2024-02-2708_11_07.834561\\model-00004-0.69127-0.73454-6.57156-0.24000.h5\n",
      "23/23 [==============================] - 143s 6s/step - loss: 0.6913 - categorical_accuracy: 0.7345 - val_loss: 6.5716 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6561 - categorical_accuracy: 0.7707\n",
      "Epoch 5: saving model to model_init_2024-02-2708_11_07.834561\\model-00005-0.65609-0.77074-6.01473-0.21000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "23/23 [==============================] - 153s 7s/step - loss: 0.6561 - categorical_accuracy: 0.7707 - val_loss: 6.0147 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=5)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "history_model  = conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model - Conv3D \n",
    "#### Image Size - 100 X 100, Batch size - 60, epochs - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 687813\n",
      "Epoch 1/3\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.8086 - categorical_accuracy: 0.3529 \n",
      "Epoch 1: saving model to model_init_2024-02-2708_24_21.744140\\model-00001-1.80860-0.35294-1.67041-0.16000.h5\n",
      "12/12 [==============================] - 184s 16s/step - loss: 1.8086 - categorical_accuracy: 0.3529 - val_loss: 1.6704 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1084 - categorical_accuracy: 0.5897 \n",
      "Epoch 2: saving model to model_init_2024-02-2708_24_21.744140\\model-00002-1.10840-0.58974-2.98459-0.16000.h5\n",
      "12/12 [==============================] - 143s 13s/step - loss: 1.1084 - categorical_accuracy: 0.5897 - val_loss: 2.9846 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9321 - categorical_accuracy: 0.6516 \n",
      "Epoch 3: saving model to model_init_2024-02-2708_24_21.744140\\model-00003-0.93214-0.65158-4.59653-0.16000.h5\n",
      "12/12 [==============================] - 156s 14s/step - loss: 0.9321 - categorical_accuracy: 0.6516 - val_loss: 4.5965 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=60,num_epochs=3)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "history_model  = conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model - Conv3D \n",
    "#### Image Size - 160 X 160, Batch size - 40, epochs - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1736389\n",
      "Epoch 1/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4920 - categorical_accuracy: 0.4540\n",
      "Epoch 1: saving model to model_init_2024-02-2708_33_01.642530\\model-00001-1.49202-0.45400-1.98490-0.16000.h5\n",
      "17/17 [==============================] - 137s 8s/step - loss: 1.4920 - categorical_accuracy: 0.4540 - val_loss: 1.9849 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6531 - categorical_accuracy: 0.7738\n",
      "Epoch 2: saving model to model_init_2024-02-2708_33_01.642530\\model-00002-0.65315-0.77376-3.23808-0.17000.h5\n",
      "17/17 [==============================] - 120s 7s/step - loss: 0.6531 - categorical_accuracy: 0.7738 - val_loss: 3.2381 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4154 - categorical_accuracy: 0.8416\n",
      "Epoch 3: saving model to model_init_2024-02-2708_33_01.642530\\model-00003-0.41539-0.84163-3.40760-0.13000.h5\n",
      "17/17 [==============================] - 110s 7s/step - loss: 0.4154 - categorical_accuracy: 0.8416 - val_loss: 3.4076 - val_categorical_accuracy: 0.1300 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2968 - categorical_accuracy: 0.9020\n",
      "Epoch 4: saving model to model_init_2024-02-2708_33_01.642530\\model-00004-0.29683-0.90196-4.03417-0.16000.h5\n",
      "17/17 [==============================] - 117s 7s/step - loss: 0.2968 - categorical_accuracy: 0.9020 - val_loss: 4.0342 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1886 - categorical_accuracy: 0.9412\n",
      "Epoch 5: saving model to model_init_2024-02-2708_33_01.642530\\model-00005-0.18858-0.94118-4.63300-0.17000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "17/17 [==============================] - 114s 7s/step - loss: 0.1886 - categorical_accuracy: 0.9412 - val_loss: 4.6330 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27f429daf50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=40,num_epochs=5)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model - Reduced filter size and image size 120 x 120, Augment Data with Dropout\n",
    "#### Image Size - 120 X 120, Batch size - 30, epochs - 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv3D3(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_24 (Conv3D)          (None, 16, 120, 120, 16   400       \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 16, 120, 120, 16   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_36 (Ba  (None, 16, 120, 120, 16   64        \n",
      " tchNormalization)           )                                   \n",
      "                                                                 \n",
      " max_pooling3d_24 (MaxPooli  (None, 8, 60, 60, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_25 (Conv3D)          (None, 8, 60, 60, 32)     4128      \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 8, 60, 60, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 8, 60, 60, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_25 (MaxPooli  (None, 4, 30, 30, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_26 (Conv3D)          (None, 4, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 4, 30, 30, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_38 (Ba  (None, 4, 30, 30, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_26 (MaxPooli  (None, 2, 15, 15, 64)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_27 (Conv3D)          (None, 2, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 2, 15, 15, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_39 (Ba  (None, 2, 15, 15, 128)    512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_27 (MaxPooli  (None, 1, 7, 7, 128)      0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               1605888   \n",
      "                                                                 \n",
      " batch_normalization_40 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_41 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1762613 (6.72 MB)\n",
      "Trainable params: 1761109 (6.72 MB)\n",
      "Non-trainable params: 1504 (5.88 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d3=ModelConv3D3()\n",
    "conv_3d3.initialize_path(project_folder)\n",
    "conv_3d3.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d3.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=30)\n",
    "conv_3d3_model=conv_3d3.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n",
    "conv_3d3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1762613\n",
      "Epoch 1/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.8435 - categorical_accuracy: 0.4178\n",
      "Epoch 1: saving model to model_init_2024-02-2708_43_40.561252\\model-00001-1.84349-0.41780-3.78421-0.16000.h5\n",
      "23/23 [==============================] - 163s 7s/step - loss: 1.8435 - categorical_accuracy: 0.4178 - val_loss: 3.7842 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1711 - categorical_accuracy: 0.5897\n",
      "Epoch 2: saving model to model_init_2024-02-2708_43_40.561252\\model-00002-1.17105-0.58974-6.72155-0.13000.h5\n",
      "23/23 [==============================] - 142s 6s/step - loss: 1.1711 - categorical_accuracy: 0.5897 - val_loss: 6.7215 - val_categorical_accuracy: 0.1300 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9148 - categorical_accuracy: 0.6644\n",
      "Epoch 3: saving model to model_init_2024-02-2708_43_40.561252\\model-00003-0.91480-0.66440-9.14666-0.14000.h5\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.9148 - categorical_accuracy: 0.6644 - val_loss: 9.1467 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7922 - categorical_accuracy: 0.7187\n",
      "Epoch 4: saving model to model_init_2024-02-2708_43_40.561252\\model-00004-0.79216-0.71870-9.47645-0.17000.h5\n",
      "23/23 [==============================] - 141s 6s/step - loss: 0.7922 - categorical_accuracy: 0.7187 - val_loss: 9.4764 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6779 - categorical_accuracy: 0.7572\n",
      "Epoch 5: saving model to model_init_2024-02-2708_43_40.561252\\model-00005-0.67791-0.75716-8.19749-0.24000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "23/23 [==============================] - 144s 6s/step - loss: 0.6779 - categorical_accuracy: 0.7572 - val_loss: 8.1975 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4641 - categorical_accuracy: 0.8250\n",
      "Epoch 6: saving model to model_init_2024-02-2708_43_40.561252\\model-00006-0.46406-0.82504-7.92112-0.20000.h5\n",
      "23/23 [==============================] - 113s 5s/step - loss: 0.4641 - categorical_accuracy: 0.8250 - val_loss: 7.9211 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4010 - categorical_accuracy: 0.8477\n",
      "Epoch 7: saving model to model_init_2024-02-2708_43_40.561252\\model-00007-0.40101-0.84766-9.11737-0.24000.h5\n",
      "23/23 [==============================] - 79s 4s/step - loss: 0.4010 - categorical_accuracy: 0.8477 - val_loss: 9.1174 - val_categorical_accuracy: 0.2400 - lr: 2.0000e-04\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3952 - categorical_accuracy: 0.8597\n",
      "Epoch 8: saving model to model_init_2024-02-2708_43_40.561252\\model-00008-0.39519-0.85973-9.34282-0.18000.h5\n",
      "23/23 [==============================] - 79s 4s/step - loss: 0.3952 - categorical_accuracy: 0.8597 - val_loss: 9.3428 - val_categorical_accuracy: 0.1800 - lr: 2.0000e-04\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3834 - categorical_accuracy: 0.8537\n",
      "Epoch 9: saving model to model_init_2024-02-2708_43_40.561252\\model-00009-0.38340-0.85370-8.32066-0.22000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "23/23 [==============================] - 80s 4s/step - loss: 0.3834 - categorical_accuracy: 0.8537 - val_loss: 8.3207 - val_categorical_accuracy: 0.2200 - lr: 2.0000e-04\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3907 - categorical_accuracy: 0.8605\n",
      "Epoch 10: saving model to model_init_2024-02-2708_43_40.561252\\model-00010-0.39071-0.86048-8.49884-0.20000.h5\n",
      "23/23 [==============================] - 80s 4s/step - loss: 0.3907 - categorical_accuracy: 0.8605 - val_loss: 8.4988 - val_categorical_accuracy: 0.2000 - lr: 4.0000e-05\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3196 - categorical_accuracy: 0.8771\n",
      "Epoch 11: saving model to model_init_2024-02-2708_43_40.561252\\model-00011-0.31955-0.87707-7.76248-0.21000.h5\n",
      "23/23 [==============================] - 72s 3s/step - loss: 0.3196 - categorical_accuracy: 0.8771 - val_loss: 7.7625 - val_categorical_accuracy: 0.2100 - lr: 4.0000e-05\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3167 - categorical_accuracy: 0.8839\n",
      "Epoch 12: saving model to model_init_2024-02-2708_43_40.561252\\model-00012-0.31666-0.88386-6.96169-0.23000.h5\n",
      "23/23 [==============================] - 70s 3s/step - loss: 0.3167 - categorical_accuracy: 0.8839 - val_loss: 6.9617 - val_categorical_accuracy: 0.2300 - lr: 4.0000e-05\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3182 - categorical_accuracy: 0.8824\n",
      "Epoch 13: saving model to model_init_2024-02-2708_43_40.561252\\model-00013-0.31815-0.88235-6.54954-0.24000.h5\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "23/23 [==============================] - 70s 3s/step - loss: 0.3182 - categorical_accuracy: 0.8824 - val_loss: 6.5495 - val_categorical_accuracy: 0.2400 - lr: 4.0000e-05\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3493 - categorical_accuracy: 0.8801\n",
      "Epoch 14: saving model to model_init_2024-02-2708_43_40.561252\\model-00014-0.34926-0.88009-5.78876-0.25000.h5\n",
      "23/23 [==============================] - 70s 3s/step - loss: 0.3493 - categorical_accuracy: 0.8801 - val_loss: 5.7888 - val_categorical_accuracy: 0.2500 - lr: 8.0000e-06\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3300 - categorical_accuracy: 0.8861\n",
      "Epoch 15: saving model to model_init_2024-02-2708_43_40.561252\\model-00015-0.32996-0.88612-5.53481-0.26000.h5\n",
      "23/23 [==============================] - 72s 3s/step - loss: 0.3300 - categorical_accuracy: 0.8861 - val_loss: 5.5348 - val_categorical_accuracy: 0.2600 - lr: 8.0000e-06\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3383 - categorical_accuracy: 0.8748\n",
      "Epoch 16: saving model to model_init_2024-02-2708_43_40.561252\\model-00016-0.33835-0.87481-4.78600-0.27000.h5\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.3383 - categorical_accuracy: 0.8748 - val_loss: 4.7860 - val_categorical_accuracy: 0.2700 - lr: 8.0000e-06\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2843 - categorical_accuracy: 0.8952\n",
      "Epoch 17: saving model to model_init_2024-02-2708_43_40.561252\\model-00017-0.28430-0.89517-4.30993-0.30000.h5\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "23/23 [==============================] - 71s 3s/step - loss: 0.2843 - categorical_accuracy: 0.8952 - val_loss: 4.3099 - val_categorical_accuracy: 0.3000 - lr: 8.0000e-06\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3120 - categorical_accuracy: 0.8869\n",
      "Epoch 18: saving model to model_init_2024-02-2708_43_40.561252\\model-00018-0.31202-0.88688-3.82496-0.32000.h5\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.3120 - categorical_accuracy: 0.8869 - val_loss: 3.8250 - val_categorical_accuracy: 0.3200 - lr: 1.6000e-06\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3663 - categorical_accuracy: 0.8597\n",
      "Epoch 19: saving model to model_init_2024-02-2708_43_40.561252\\model-00019-0.36634-0.85973-3.51015-0.33000.h5\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.3663 - categorical_accuracy: 0.8597 - val_loss: 3.5102 - val_categorical_accuracy: 0.3300 - lr: 1.6000e-06\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3372 - categorical_accuracy: 0.8725\n",
      "Epoch 20: saving model to model_init_2024-02-2708_43_40.561252\\model-00020-0.33717-0.87255-2.49033-0.38000.h5\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.3372 - categorical_accuracy: 0.8725 - val_loss: 2.4903 - val_categorical_accuracy: 0.3800 - lr: 1.6000e-06\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2827 - categorical_accuracy: 0.8959\n",
      "Epoch 21: saving model to model_init_2024-02-2708_43_40.561252\\model-00021-0.28273-0.89593-2.22651-0.41000.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 79s 4s/step - loss: 0.2827 - categorical_accuracy: 0.8959 - val_loss: 2.2265 - val_categorical_accuracy: 0.4100 - lr: 1.6000e-06\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3280 - categorical_accuracy: 0.8854\n",
      "Epoch 22: saving model to model_init_2024-02-2708_43_40.561252\\model-00022-0.32798-0.88537-1.72976-0.50000.h5\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.3280 - categorical_accuracy: 0.8854 - val_loss: 1.7298 - val_categorical_accuracy: 0.5000 - lr: 1.6000e-06\n",
      "Epoch 23/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3226 - categorical_accuracy: 0.8778\n",
      "Epoch 23: saving model to model_init_2024-02-2708_43_40.561252\\model-00023-0.32258-0.87783-1.63166-0.51000.h5\n",
      "23/23 [==============================] - 70s 3s/step - loss: 0.3226 - categorical_accuracy: 0.8778 - val_loss: 1.6317 - val_categorical_accuracy: 0.5100 - lr: 1.6000e-06\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3191 - categorical_accuracy: 0.8891\n",
      "Epoch 24: saving model to model_init_2024-02-2708_43_40.561252\\model-00024-0.31908-0.88914-1.17646-0.63000.h5\n",
      "23/23 [==============================] - 68s 3s/step - loss: 0.3191 - categorical_accuracy: 0.8891 - val_loss: 1.1765 - val_categorical_accuracy: 0.6300 - lr: 1.6000e-06\n",
      "Epoch 25/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3161 - categorical_accuracy: 0.8831\n",
      "Epoch 25: saving model to model_init_2024-02-2708_43_40.561252\\model-00025-0.31611-0.88311-0.99687-0.67000.h5\n",
      "23/23 [==============================] - 71s 3s/step - loss: 0.3161 - categorical_accuracy: 0.8831 - val_loss: 0.9969 - val_categorical_accuracy: 0.6700 - lr: 1.6000e-06\n",
      "Epoch 26/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3045 - categorical_accuracy: 0.8876\n",
      "Epoch 26: saving model to model_init_2024-02-2708_43_40.561252\\model-00026-0.30446-0.88763-0.75108-0.73000.h5\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.3045 - categorical_accuracy: 0.8876 - val_loss: 0.7511 - val_categorical_accuracy: 0.7300 - lr: 1.6000e-06\n",
      "Epoch 27/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3057 - categorical_accuracy: 0.8861\n",
      "Epoch 27: saving model to model_init_2024-02-2708_43_40.561252\\model-00027-0.30565-0.88612-0.69361-0.79000.h5\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.3057 - categorical_accuracy: 0.8861 - val_loss: 0.6936 - val_categorical_accuracy: 0.7900 - lr: 1.6000e-06\n",
      "Epoch 28/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2968 - categorical_accuracy: 0.8876\n",
      "Epoch 28: saving model to model_init_2024-02-2708_43_40.561252\\model-00028-0.29680-0.88763-0.53766-0.81000.h5\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.2968 - categorical_accuracy: 0.8876 - val_loss: 0.5377 - val_categorical_accuracy: 0.8100 - lr: 1.6000e-06\n",
      "Epoch 29/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2991 - categorical_accuracy: 0.8997\n",
      "Epoch 29: saving model to model_init_2024-02-2708_43_40.561252\\model-00029-0.29905-0.89970-0.52930-0.82000.h5\n",
      "23/23 [==============================] - 70s 3s/step - loss: 0.2991 - categorical_accuracy: 0.8997 - val_loss: 0.5293 - val_categorical_accuracy: 0.8200 - lr: 1.6000e-06\n",
      "Epoch 30/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3072 - categorical_accuracy: 0.8891\n",
      "Epoch 30: saving model to model_init_2024-02-2708_43_40.561252\\model-00030-0.30720-0.88914-0.51198-0.82000.h5\n",
      "23/23 [==============================] - 68s 3s/step - loss: 0.3072 - categorical_accuracy: 0.8891 - val_loss: 0.5120 - val_categorical_accuracy: 0.8200 - lr: 1.6000e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d3_model.count_params())\n",
    "history_model3=conv_3d3.train_model(conv_3d3_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model - Added few more layers along with dropout \n",
    "#### Image Size - 120 X 120, Batch size - 20, epochs - 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " class ModelConv3D4(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_28 (Conv3D)          (None, 16, 120, 120, 16   1312      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 16, 120, 120, 16   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_42 (Ba  (None, 16, 120, 120, 16   64        \n",
      " tchNormalization)           )                                   \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 16, 120, 120, 16   6928      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 16, 120, 120, 16   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_43 (Ba  (None, 16, 120, 120, 16   64        \n",
      " tchNormalization)           )                                   \n",
      "                                                                 \n",
      " max_pooling3d_28 (MaxPooli  (None, 8, 60, 60, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_30 (Conv3D)          (None, 8, 60, 60, 32)     13856     \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 8, 60, 60, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_44 (Ba  (None, 8, 60, 60, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3d_31 (Conv3D)          (None, 8, 60, 60, 32)     27680     \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 8, 60, 60, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_45 (Ba  (None, 8, 60, 60, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_29 (MaxPooli  (None, 4, 30, 30, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_32 (Conv3D)          (None, 4, 30, 30, 64)     55360     \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 4, 30, 30, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_46 (Ba  (None, 4, 30, 30, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3d_33 (Conv3D)          (None, 4, 30, 30, 64)     110656    \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 4, 30, 30, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_47 (Ba  (None, 4, 30, 30, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_30 (MaxPooli  (None, 2, 15, 15, 64)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_34 (Conv3D)          (None, 2, 15, 15, 128)    221312    \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 2, 15, 15, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_48 (Ba  (None, 2, 15, 15, 128)    512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3d_35 (Conv3D)          (None, 2, 15, 15, 128)    442496    \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 2, 15, 15, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_49 (Ba  (None, 2, 15, 15, 128)    512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_31 (MaxPooli  (None, 1, 7, 7, 128)      0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               1605888   \n",
      "                                                                 \n",
      " batch_normalization_50 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_51 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2556533 (9.75 MB)\n",
      "Trainable params: 2554549 (9.74 MB)\n",
      "Non-trainable params: 1984 (7.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d4=ModelConv3D4()\n",
    "conv_3d4.initialize_path(project_folder)\n",
    "conv_3d4.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d4.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=10)\n",
    "conv_3d4_model=conv_3d4.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n",
    "conv_3d4_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 2556533\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.8691 - categorical_accuracy: 0.4020\n",
      "Epoch 1: saving model to model_init_2024-02-2709_28_50.190618\\model-00001-1.86910-0.40196-3.00819-0.22000.h5\n",
      "34/34 [==============================] - 205s 6s/step - loss: 1.8691 - categorical_accuracy: 0.4020 - val_loss: 3.0082 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4222 - categorical_accuracy: 0.5068\n",
      "Epoch 2: saving model to model_init_2024-02-2709_28_50.190618\\model-00002-1.42217-0.50679-3.96332-0.22000.h5\n",
      "34/34 [==============================] - 191s 6s/step - loss: 1.4222 - categorical_accuracy: 0.5068 - val_loss: 3.9633 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3049 - categorical_accuracy: 0.5588\n",
      "Epoch 3: saving model to model_init_2024-02-2709_28_50.190618\\model-00003-1.30490-0.55882-4.33317-0.23000.h5\n",
      "34/34 [==============================] - 188s 6s/step - loss: 1.3049 - categorical_accuracy: 0.5588 - val_loss: 4.3332 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0570 - categorical_accuracy: 0.6523\n",
      "Epoch 4: saving model to model_init_2024-02-2709_28_50.190618\\model-00004-1.05699-0.65234-2.16044-0.29000.h5\n",
      "34/34 [==============================] - 188s 6s/step - loss: 1.0570 - categorical_accuracy: 0.6523 - val_loss: 2.1604 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8721 - categorical_accuracy: 0.6795\n",
      "Epoch 5: saving model to model_init_2024-02-2709_28_50.190618\\model-00005-0.87205-0.67949-2.36328-0.27000.h5\n",
      "34/34 [==============================] - 188s 6s/step - loss: 0.8721 - categorical_accuracy: 0.6795 - val_loss: 2.3633 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8323 - categorical_accuracy: 0.7119\n",
      "Epoch 6: saving model to model_init_2024-02-2709_28_50.190618\\model-00006-0.83231-0.71192-3.62561-0.30000.h5\n",
      "34/34 [==============================] - 189s 6s/step - loss: 0.8323 - categorical_accuracy: 0.7119 - val_loss: 3.6256 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6962 - categorical_accuracy: 0.7511\n",
      "Epoch 7: saving model to model_init_2024-02-2709_28_50.190618\\model-00007-0.69623-0.75113-3.54304-0.36000.h5\n",
      "34/34 [==============================] - 189s 6s/step - loss: 0.6962 - categorical_accuracy: 0.7511 - val_loss: 3.5430 - val_categorical_accuracy: 0.3600 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5126 - categorical_accuracy: 0.8183\n",
      "Epoch 8: saving model to model_init_2024-02-2709_28_50.190618\\model-00008-0.51256-0.81825-3.03781-0.39000.h5\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "34/34 [==============================] - 189s 6s/step - loss: 0.5126 - categorical_accuracy: 0.8183 - val_loss: 3.0378 - val_categorical_accuracy: 0.3900 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4514 - categorical_accuracy: 0.8326\n",
      "Epoch 9: saving model to model_init_2024-02-2709_28_50.190618\\model-00009-0.45143-0.83258-3.56101-0.27000.h5\n",
      "34/34 [==============================] - 195s 6s/step - loss: 0.4514 - categorical_accuracy: 0.8326 - val_loss: 3.5610 - val_categorical_accuracy: 0.2700 - lr: 2.0000e-04\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3778 - categorical_accuracy: 0.8643\n",
      "Epoch 10: saving model to model_init_2024-02-2709_28_50.190618\\model-00010-0.37775-0.86425-2.52589-0.33000.h5\n",
      "34/34 [==============================] - 193s 6s/step - loss: 0.3778 - categorical_accuracy: 0.8643 - val_loss: 2.5259 - val_categorical_accuracy: 0.3300 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d4_model.count_params())\n",
    "history_model4=conv_3d4.train_model(conv_3d4_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model - Added dropout at convolution layers\n",
    "#### Image Size - 120 X 120, Batch size - 20, epochs - 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv3D5(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_36 (Conv3D)          (None, 16, 120, 120, 16   1312      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 16, 120, 120, 16   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_57 (Ba  (None, 16, 120, 120, 16   64        \n",
      " tchNormalization)           )                                   \n",
      "                                                                 \n",
      " conv3d_37 (Conv3D)          (None, 16, 120, 120, 16   6928      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 16, 120, 120, 16   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_58 (Ba  (None, 16, 120, 120, 16   64        \n",
      " tchNormalization)           )                                   \n",
      "                                                                 \n",
      " max_pooling3d_32 (MaxPooli  (None, 8, 60, 60, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 8, 60, 60, 16)     0         \n",
      "                                                                 \n",
      " conv3d_38 (Conv3D)          (None, 8, 60, 60, 32)     13856     \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 8, 60, 60, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_59 (Ba  (None, 8, 60, 60, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3d_39 (Conv3D)          (None, 8, 60, 60, 32)     27680     \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 8, 60, 60, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_60 (Ba  (None, 8, 60, 60, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_33 (MaxPooli  (None, 4, 30, 30, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 4, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " conv3d_40 (Conv3D)          (None, 4, 30, 30, 64)     55360     \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 4, 30, 30, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_61 (Ba  (None, 4, 30, 30, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3d_41 (Conv3D)          (None, 4, 30, 30, 64)     110656    \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 4, 30, 30, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_62 (Ba  (None, 4, 30, 30, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_34 (MaxPooli  (None, 2, 15, 15, 64)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 2, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " conv3d_42 (Conv3D)          (None, 2, 15, 15, 128)    221312    \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 2, 15, 15, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_63 (Ba  (None, 2, 15, 15, 128)    512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3d_43 (Conv3D)          (None, 2, 15, 15, 128)    442496    \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 2, 15, 15, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_64 (Ba  (None, 2, 15, 15, 128)    512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_35 (MaxPooli  (None, 1, 7, 7, 128)      0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 1, 7, 7, 128)      0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               1605888   \n",
      "                                                                 \n",
      " batch_normalization_65 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_66 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2556533 (9.75 MB)\n",
      "Trainable params: 2554549 (9.74 MB)\n",
      "Non-trainable params: 1984 (7.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d5=ModelConv3D5()\n",
    "conv_3d5.initialize_path(project_folder)\n",
    "conv_3d5.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d5.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=20)\n",
    "conv_3d5_model=conv_3d5.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.25)\n",
    "conv_3d5_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 2556533\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7472 - categorical_accuracy: 0.3967\n",
      "Epoch 1: saving model to model_init_2024-02-2712_51_11.850275\\model-00001-1.74718-0.39668-1.68901-0.24000.h5\n",
      "34/34 [==============================] - 200s 6s/step - loss: 1.7472 - categorical_accuracy: 0.3967 - val_loss: 1.6890 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3092 - categorical_accuracy: 0.5204\n",
      "Epoch 2: saving model to model_init_2024-02-2712_51_11.850275\\model-00002-1.30920-0.52036-1.94681-0.21000.h5\n",
      "34/34 [==============================] - 190s 6s/step - loss: 1.3092 - categorical_accuracy: 0.5204 - val_loss: 1.9468 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0475 - categorical_accuracy: 0.6086\n",
      "Epoch 3: saving model to model_init_2024-02-2712_51_11.850275\\model-00003-1.04746-0.60860-2.57951-0.15000.h5\n",
      "34/34 [==============================] - 190s 6s/step - loss: 1.0475 - categorical_accuracy: 0.6086 - val_loss: 2.5795 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8585 - categorical_accuracy: 0.6712\n",
      "Epoch 4: saving model to model_init_2024-02-2712_51_11.850275\\model-00004-0.85850-0.67119-1.77112-0.26000.h5\n",
      "34/34 [==============================] - 191s 6s/step - loss: 0.8585 - categorical_accuracy: 0.6712 - val_loss: 1.7711 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6882 - categorical_accuracy: 0.7436\n",
      "Epoch 5: saving model to model_init_2024-02-2712_51_11.850275\\model-00005-0.68822-0.74359-3.40057-0.23000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "34/34 [==============================] - 192s 6s/step - loss: 0.6882 - categorical_accuracy: 0.7436 - val_loss: 3.4006 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4738 - categorical_accuracy: 0.8205\n",
      "Epoch 6: saving model to model_init_2024-02-2712_51_11.850275\\model-00006-0.47383-0.82051-2.98881-0.23000.h5\n",
      "34/34 [==============================] - 193s 6s/step - loss: 0.4738 - categorical_accuracy: 0.8205 - val_loss: 2.9888 - val_categorical_accuracy: 0.2300 - lr: 2.0000e-04\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3917 - categorical_accuracy: 0.8469\n",
      "Epoch 7: saving model to model_init_2024-02-2712_51_11.850275\\model-00007-0.39169-0.84691-3.73582-0.21000.h5\n",
      "34/34 [==============================] - 192s 6s/step - loss: 0.3917 - categorical_accuracy: 0.8469 - val_loss: 3.7358 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3364 - categorical_accuracy: 0.8741\n",
      "Epoch 8: saving model to model_init_2024-02-2712_51_11.850275\\model-00008-0.33642-0.87406-3.91103-0.24000.h5\n",
      "34/34 [==============================] - 192s 6s/step - loss: 0.3364 - categorical_accuracy: 0.8741 - val_loss: 3.9110 - val_categorical_accuracy: 0.2400 - lr: 2.0000e-04\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3442 - categorical_accuracy: 0.8718\n",
      "Epoch 9: saving model to model_init_2024-02-2712_51_11.850275\\model-00009-0.34416-0.87179-4.36644-0.25000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "34/34 [==============================] - 192s 6s/step - loss: 0.3442 - categorical_accuracy: 0.8718 - val_loss: 4.3664 - val_categorical_accuracy: 0.2500 - lr: 2.0000e-04\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3153 - categorical_accuracy: 0.8733\n",
      "Epoch 10: saving model to model_init_2024-02-2712_51_11.850275\\model-00010-0.31526-0.87330-4.12425-0.23000.h5\n",
      "34/34 [==============================] - 192s 6s/step - loss: 0.3153 - categorical_accuracy: 0.8733 - val_loss: 4.1243 - val_categorical_accuracy: 0.2300 - lr: 4.0000e-05\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2860 - categorical_accuracy: 0.8922\n",
      "Epoch 11: saving model to model_init_2024-02-2712_51_11.850275\\model-00011-0.28600-0.89216-3.71400-0.22000.h5\n",
      "34/34 [==============================] - 194s 6s/step - loss: 0.2860 - categorical_accuracy: 0.8922 - val_loss: 3.7140 - val_categorical_accuracy: 0.2200 - lr: 4.0000e-05\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2773 - categorical_accuracy: 0.9005\n",
      "Epoch 12: saving model to model_init_2024-02-2712_51_11.850275\\model-00012-0.27725-0.90045-3.56517-0.21000.h5\n",
      "34/34 [==============================] - 190s 6s/step - loss: 0.2773 - categorical_accuracy: 0.9005 - val_loss: 3.5652 - val_categorical_accuracy: 0.2100 - lr: 4.0000e-05\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2618 - categorical_accuracy: 0.9005\n",
      "Epoch 13: saving model to model_init_2024-02-2712_51_11.850275\\model-00013-0.26178-0.90045-2.79266-0.27000.h5\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "34/34 [==============================] - 194s 6s/step - loss: 0.2618 - categorical_accuracy: 0.9005 - val_loss: 2.7927 - val_categorical_accuracy: 0.2700 - lr: 4.0000e-05\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2654 - categorical_accuracy: 0.8967\n",
      "Epoch 14: saving model to model_init_2024-02-2712_51_11.850275\\model-00014-0.26544-0.89668-2.39832-0.39000.h5\n",
      "34/34 [==============================] - 191s 6s/step - loss: 0.2654 - categorical_accuracy: 0.8967 - val_loss: 2.3983 - val_categorical_accuracy: 0.3900 - lr: 8.0000e-06\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2653 - categorical_accuracy: 0.9020\n",
      "Epoch 15: saving model to model_init_2024-02-2712_51_11.850275\\model-00015-0.26535-0.90196-2.32775-0.34000.h5\n",
      "34/34 [==============================] - 191s 6s/step - loss: 0.2653 - categorical_accuracy: 0.9020 - val_loss: 2.3277 - val_categorical_accuracy: 0.3400 - lr: 8.0000e-06\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2316 - categorical_accuracy: 0.9201\n",
      "Epoch 16: saving model to model_init_2024-02-2712_51_11.850275\\model-00016-0.23162-0.92006-1.72793-0.46000.h5\n",
      "34/34 [==============================] - 191s 6s/step - loss: 0.2316 - categorical_accuracy: 0.9201 - val_loss: 1.7279 - val_categorical_accuracy: 0.4600 - lr: 8.0000e-06\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2449 - categorical_accuracy: 0.9095\n",
      "Epoch 17: saving model to model_init_2024-02-2712_51_11.850275\\model-00017-0.24486-0.90950-1.64358-0.47000.h5\n",
      "34/34 [==============================] - 191s 6s/step - loss: 0.2449 - categorical_accuracy: 0.9095 - val_loss: 1.6436 - val_categorical_accuracy: 0.4700 - lr: 8.0000e-06\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2595 - categorical_accuracy: 0.8952\n",
      "Epoch 18: saving model to model_init_2024-02-2712_51_11.850275\\model-00018-0.25947-0.89517-1.13493-0.58000.h5\n",
      "34/34 [==============================] - 229s 7s/step - loss: 0.2595 - categorical_accuracy: 0.8952 - val_loss: 1.1349 - val_categorical_accuracy: 0.5800 - lr: 8.0000e-06\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2323 - categorical_accuracy: 0.9208 \n",
      "Epoch 19: saving model to model_init_2024-02-2712_51_11.850275\\model-00019-0.23226-0.92081-1.30958-0.56000.h5\n",
      "34/34 [==============================] - 362s 11s/step - loss: 0.2323 - categorical_accuracy: 0.9208 - val_loss: 1.3096 - val_categorical_accuracy: 0.5600 - lr: 8.0000e-06\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2527 - categorical_accuracy: 0.9057 \n",
      "Epoch 20: saving model to model_init_2024-02-2712_51_11.850275\\model-00020-0.25268-0.90573-1.02876-0.66000.h5\n",
      "34/34 [==============================] - 362s 11s/step - loss: 0.2527 - categorical_accuracy: 0.9057 - val_loss: 1.0288 - val_categorical_accuracy: 0.6600 - lr: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d5_model.count_params())\n",
    "history_model5=conv_3d5.train_model(conv_3d5_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model - Reduced the number of parameters  \n",
    "#### Image Size - 120 X 120, Batch size - 20, epochs - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv3D7(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_44 (Conv3D)          (None, 16, 120, 120, 16   1312      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 16, 120, 120, 16   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_67 (Ba  (None, 16, 120, 120, 16   64        \n",
      " tchNormalization)           )                                   \n",
      "                                                                 \n",
      " max_pooling3d_36 (MaxPooli  (None, 8, 60, 60, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_45 (Conv3D)          (None, 8, 60, 60, 32)     13856     \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 8, 60, 60, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_68 (Ba  (None, 8, 60, 60, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_37 (MaxPooli  (None, 4, 30, 30, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_46 (Conv3D)          (None, 4, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 4, 30, 30, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_69 (Ba  (None, 4, 30, 30, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_38 (MaxPooli  (None, 2, 15, 15, 64)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_47 (Conv3D)          (None, 2, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 2, 15, 15, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_70 (Ba  (None, 2, 15, 15, 128)    512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling3d_39 (MaxPooli  (None, 1, 7, 7, 128)      0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                401472    \n",
      "                                                                 \n",
      " batch_normalization_71 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_72 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 504709 (1.93 MB)\n",
      "Trainable params: 503973 (1.92 MB)\n",
      "Non-trainable params: 736 (2.88 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d7=ModelConv3D7()\n",
    "conv_3d7.initialize_path(project_folder)\n",
    "conv_3d7.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d7.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=20)\n",
    "conv_3d7_model=conv_3d7.define_model(dense_neurons=64,dropout=0.25)\n",
    "conv_3d7_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 504709\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5341 - categorical_accuracy: 0.4412\n",
      "Epoch 1: saving model to model_init_2024-02-2714_03_41.008245\\model-00001-1.53409-0.44118-2.65844-0.16000.h5\n",
      "34/34 [==============================] - 169s 5s/step - loss: 1.5341 - categorical_accuracy: 0.4412 - val_loss: 2.6584 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9941 - categorical_accuracy: 0.6267\n",
      "Epoch 2: saving model to model_init_2024-02-2714_03_41.008245\\model-00002-0.99407-0.62670-4.12856-0.16000.h5\n",
      "34/34 [==============================] - 162s 5s/step - loss: 0.9941 - categorical_accuracy: 0.6267 - val_loss: 4.1286 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7872 - categorical_accuracy: 0.7051\n",
      "Epoch 3: saving model to model_init_2024-02-2714_03_41.008245\\model-00003-0.78718-0.70513-4.02086-0.29000.h5\n",
      "34/34 [==============================] - 167s 5s/step - loss: 0.7872 - categorical_accuracy: 0.7051 - val_loss: 4.0209 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6382 - categorical_accuracy: 0.7609\n",
      "Epoch 4: saving model to model_init_2024-02-2714_03_41.008245\\model-00004-0.63825-0.76094-4.11118-0.24000.h5\n",
      "34/34 [==============================] - 154s 5s/step - loss: 0.6382 - categorical_accuracy: 0.7609 - val_loss: 4.1112 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5085 - categorical_accuracy: 0.8137\n",
      "Epoch 5: saving model to model_init_2024-02-2714_03_41.008245\\model-00005-0.50848-0.81373-3.23506-0.24000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "34/34 [==============================] - 156s 5s/step - loss: 0.5085 - categorical_accuracy: 0.8137 - val_loss: 3.2351 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3725 - categorical_accuracy: 0.8612\n",
      "Epoch 6: saving model to model_init_2024-02-2714_03_41.008245\\model-00006-0.37251-0.86124-3.30670-0.27000.h5\n",
      "34/34 [==============================] - 156s 5s/step - loss: 0.3725 - categorical_accuracy: 0.8612 - val_loss: 3.3067 - val_categorical_accuracy: 0.2700 - lr: 2.0000e-04\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3693 - categorical_accuracy: 0.8793\n",
      "Epoch 7: saving model to model_init_2024-02-2714_03_41.008245\\model-00007-0.36934-0.87934-2.76128-0.30000.h5\n",
      "34/34 [==============================] - 160s 5s/step - loss: 0.3693 - categorical_accuracy: 0.8793 - val_loss: 2.7613 - val_categorical_accuracy: 0.3000 - lr: 2.0000e-04\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2959 - categorical_accuracy: 0.9020\n",
      "Epoch 8: saving model to model_init_2024-02-2714_03_41.008245\\model-00008-0.29594-0.90196-3.00568-0.23000.h5\n",
      "34/34 [==============================] - 156s 5s/step - loss: 0.2959 - categorical_accuracy: 0.9020 - val_loss: 3.0057 - val_categorical_accuracy: 0.2300 - lr: 2.0000e-04\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2990 - categorical_accuracy: 0.9057\n",
      "Epoch 9: saving model to model_init_2024-02-2714_03_41.008245\\model-00009-0.29895-0.90573-1.96819-0.37000.h5\n",
      "34/34 [==============================] - 161s 5s/step - loss: 0.2990 - categorical_accuracy: 0.9057 - val_loss: 1.9682 - val_categorical_accuracy: 0.3700 - lr: 2.0000e-04\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2674 - categorical_accuracy: 0.9118\n",
      "Epoch 10: saving model to model_init_2024-02-2714_03_41.008245\\model-00010-0.26744-0.91176-1.87999-0.29000.h5\n",
      "34/34 [==============================] - 155s 5s/step - loss: 0.2674 - categorical_accuracy: 0.9118 - val_loss: 1.8800 - val_categorical_accuracy: 0.2900 - lr: 2.0000e-04\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2483 - categorical_accuracy: 0.9261\n",
      "Epoch 11: saving model to model_init_2024-02-2714_03_41.008245\\model-00011-0.24833-0.92609-1.61907-0.43000.h5\n",
      "34/34 [==============================] - 152s 5s/step - loss: 0.2483 - categorical_accuracy: 0.9261 - val_loss: 1.6191 - val_categorical_accuracy: 0.4300 - lr: 2.0000e-04\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2350 - categorical_accuracy: 0.9336\n",
      "Epoch 12: saving model to model_init_2024-02-2714_03_41.008245\\model-00012-0.23502-0.93363-1.66090-0.39000.h5\n",
      "34/34 [==============================] - 154s 5s/step - loss: 0.2350 - categorical_accuracy: 0.9336 - val_loss: 1.6609 - val_categorical_accuracy: 0.3900 - lr: 2.0000e-04\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2385 - categorical_accuracy: 0.9238\n",
      "Epoch 13: saving model to model_init_2024-02-2714_03_41.008245\\model-00013-0.23851-0.92383-1.23521-0.56000.h5\n",
      "34/34 [==============================] - 160s 5s/step - loss: 0.2385 - categorical_accuracy: 0.9238 - val_loss: 1.2352 - val_categorical_accuracy: 0.5600 - lr: 2.0000e-04\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2385 - categorical_accuracy: 0.9201\n",
      "Epoch 14: saving model to model_init_2024-02-2714_03_41.008245\\model-00014-0.23846-0.92006-0.87830-0.68000.h5\n",
      "34/34 [==============================] - 160s 5s/step - loss: 0.2385 - categorical_accuracy: 0.9201 - val_loss: 0.8783 - val_categorical_accuracy: 0.6800 - lr: 2.0000e-04\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2258 - categorical_accuracy: 0.9344\n",
      "Epoch 15: saving model to model_init_2024-02-2714_03_41.008245\\model-00015-0.22583-0.93439-0.53753-0.77000.h5\n",
      "34/34 [==============================] - 161s 5s/step - loss: 0.2258 - categorical_accuracy: 0.9344 - val_loss: 0.5375 - val_categorical_accuracy: 0.7700 - lr: 2.0000e-04\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2148 - categorical_accuracy: 0.9344\n",
      "Epoch 16: saving model to model_init_2024-02-2714_03_41.008245\\model-00016-0.21484-0.93439-0.84463-0.68000.h5\n",
      "34/34 [==============================] - 161s 5s/step - loss: 0.2148 - categorical_accuracy: 0.9344 - val_loss: 0.8446 - val_categorical_accuracy: 0.6800 - lr: 2.0000e-04\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1478 - categorical_accuracy: 0.9608\n",
      "Epoch 17: saving model to model_init_2024-02-2714_03_41.008245\\model-00017-0.14777-0.96078-0.61611-0.77000.h5\n",
      "34/34 [==============================] - 160s 5s/step - loss: 0.1478 - categorical_accuracy: 0.9608 - val_loss: 0.6161 - val_categorical_accuracy: 0.7700 - lr: 2.0000e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1909 - categorical_accuracy: 0.9397\n",
      "Epoch 18: saving model to model_init_2024-02-2714_03_41.008245\\model-00018-0.19087-0.93967-0.61885-0.80000.h5\n",
      "34/34 [==============================] - 153s 5s/step - loss: 0.1909 - categorical_accuracy: 0.9397 - val_loss: 0.6188 - val_categorical_accuracy: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1858 - categorical_accuracy: 0.9427\n",
      "Epoch 19: saving model to model_init_2024-02-2714_03_41.008245\\model-00019-0.18584-0.94268-0.35194-0.86000.h5\n",
      "34/34 [==============================] - 153s 5s/step - loss: 0.1858 - categorical_accuracy: 0.9427 - val_loss: 0.3519 - val_categorical_accuracy: 0.8600 - lr: 2.0000e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1856 - categorical_accuracy: 0.9397\n",
      "Epoch 20: saving model to model_init_2024-02-2714_03_41.008245\\model-00020-0.18556-0.93967-0.45062-0.84000.h5\n",
      "34/34 [==============================] - 155s 5s/step - loss: 0.1856 - categorical_accuracy: 0.9397 - val_loss: 0.4506 - val_categorical_accuracy: 0.8400 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d7_model.count_params())\n",
    "history_model7=conv_3d7.train_model(conv_3d7_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model - CNN + LSTM\n",
    "#### Image Size - 120 X 120, Batch size - 20, epochs - 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "class RNNCNN1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "      \n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(LSTM(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDist  (None, 18, 120, 120, 16   448       \n",
      " ributed)                    )                                   \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 18, 120, 120, 16   64        \n",
      " stributed)                  )                                   \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 18, 60, 60, 16)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 18, 60, 60, 32)    4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDi  (None, 18, 60, 60, 32)    128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDi  (None, 18, 30, 30, 32)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 18, 30, 30, 64)    18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 18, 30, 30, 64)    256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, 18, 15, 15, 64)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDi  (None, 18, 15, 15, 128)   73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_10 (TimeD  (None, 18, 15, 15, 128)   512       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_11 (TimeD  (None, 18, 7, 7, 128)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_12 (TimeD  (None, 18, 7, 7, 256)     295168    \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_13 (TimeD  (None, 18, 7, 7, 256)     1024      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_14 (TimeD  (None, 18, 3, 3, 256)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_15 (TimeD  (None, 18, 2304)          0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               1245696   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1657445 (6.32 MB)\n",
      "Trainable params: 1656453 (6.32 MB)\n",
      "Non-trainable params: 992 (3.88 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn1=RNNCNN1()\n",
    "rnn_cnn1.initialize_path(project_folder)\n",
    "rnn_cnn1.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
    "rnn_cnn1_model=rnn_cnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1657445\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3508 - categorical_accuracy: 0.4367\n",
      "Epoch 1: saving model to model_init_2024-02-2710_01_40.642789\\model-00001-1.35085-0.43665-1.74414-0.21000.h5\n",
      "34/34 [==============================] - 120s 3s/step - loss: 1.3508 - categorical_accuracy: 0.4367 - val_loss: 1.7441 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0036 - categorical_accuracy: 0.6169\n",
      "Epoch 2: saving model to model_init_2024-02-2710_01_40.642789\\model-00002-1.00362-0.61689-2.46218-0.22000.h5\n",
      "34/34 [==============================] - 205s 6s/step - loss: 1.0036 - categorical_accuracy: 0.6169 - val_loss: 2.4622 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8198 - categorical_accuracy: 0.6817\n",
      "Epoch 3: saving model to model_init_2024-02-2710_01_40.642789\\model-00003-0.81983-0.68175-2.32708-0.21000.h5\n",
      "34/34 [==============================] - 205s 6s/step - loss: 0.8198 - categorical_accuracy: 0.6817 - val_loss: 2.3271 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6549 - categorical_accuracy: 0.7572\n",
      "Epoch 4: saving model to model_init_2024-02-2710_01_40.642789\\model-00004-0.65487-0.75716-1.76892-0.24000.h5\n",
      "34/34 [==============================] - 201s 6s/step - loss: 0.6549 - categorical_accuracy: 0.7572 - val_loss: 1.7689 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5578 - categorical_accuracy: 0.7971\n",
      "Epoch 5: saving model to model_init_2024-02-2710_01_40.642789\\model-00005-0.55782-0.79713-2.51204-0.19000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.5578 - categorical_accuracy: 0.7971 - val_loss: 2.5120 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4018 - categorical_accuracy: 0.8627\n",
      "Epoch 6: saving model to model_init_2024-02-2710_01_40.642789\\model-00006-0.40177-0.86275-2.39755-0.22000.h5\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.4018 - categorical_accuracy: 0.8627 - val_loss: 2.3976 - val_categorical_accuracy: 0.2200 - lr: 2.0000e-04\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3163 - categorical_accuracy: 0.8959\n",
      "Epoch 7: saving model to model_init_2024-02-2710_01_40.642789\\model-00007-0.31625-0.89593-2.22281-0.29000.h5\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.3163 - categorical_accuracy: 0.8959 - val_loss: 2.2228 - val_categorical_accuracy: 0.2900 - lr: 2.0000e-04\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2608 - categorical_accuracy: 0.9148\n",
      "Epoch 8: saving model to model_init_2024-02-2710_01_40.642789\\model-00008-0.26083-0.91478-2.24064-0.36000.h5\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.2608 - categorical_accuracy: 0.9148 - val_loss: 2.2406 - val_categorical_accuracy: 0.3600 - lr: 2.0000e-04\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2067 - categorical_accuracy: 0.9389\n",
      "Epoch 9: saving model to model_init_2024-02-2710_01_40.642789\\model-00009-0.20675-0.93891-2.22810-0.32000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.2067 - categorical_accuracy: 0.9389 - val_loss: 2.2281 - val_categorical_accuracy: 0.3200 - lr: 2.0000e-04\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1760 - categorical_accuracy: 0.9449\n",
      "Epoch 10: saving model to model_init_2024-02-2710_01_40.642789\\model-00010-0.17603-0.94495-2.30264-0.33000.h5\n",
      "34/34 [==============================] - 214s 6s/step - loss: 0.1760 - categorical_accuracy: 0.9449 - val_loss: 2.3026 - val_categorical_accuracy: 0.3300 - lr: 4.0000e-05\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1547 - categorical_accuracy: 0.9548\n",
      "Epoch 11: saving model to model_init_2024-02-2710_01_40.642789\\model-00011-0.15470-0.95475-2.32909-0.34000.h5\n",
      "34/34 [==============================] - 213s 6s/step - loss: 0.1547 - categorical_accuracy: 0.9548 - val_loss: 2.3291 - val_categorical_accuracy: 0.3400 - lr: 4.0000e-05\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1535 - categorical_accuracy: 0.9593\n",
      "Epoch 12: saving model to model_init_2024-02-2710_01_40.642789\\model-00012-0.15354-0.95928-2.38271-0.38000.h5\n",
      "34/34 [==============================] - 211s 6s/step - loss: 0.1535 - categorical_accuracy: 0.9593 - val_loss: 2.3827 - val_categorical_accuracy: 0.3800 - lr: 4.0000e-05\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1492 - categorical_accuracy: 0.9578\n",
      "Epoch 13: saving model to model_init_2024-02-2710_01_40.642789\\model-00013-0.14915-0.95777-2.18874-0.39000.h5\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "34/34 [==============================] - 209s 6s/step - loss: 0.1492 - categorical_accuracy: 0.9578 - val_loss: 2.1887 - val_categorical_accuracy: 0.3900 - lr: 4.0000e-05\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1438 - categorical_accuracy: 0.9608\n",
      "Epoch 14: saving model to model_init_2024-02-2710_01_40.642789\\model-00014-0.14379-0.96078-1.81552-0.45000.h5\n",
      "34/34 [==============================] - 212s 6s/step - loss: 0.1438 - categorical_accuracy: 0.9608 - val_loss: 1.8155 - val_categorical_accuracy: 0.4500 - lr: 8.0000e-06\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1424 - categorical_accuracy: 0.9570\n",
      "Epoch 15: saving model to model_init_2024-02-2710_01_40.642789\\model-00015-0.14243-0.95701-1.62795-0.50000.h5\n",
      "34/34 [==============================] - 218s 6s/step - loss: 0.1424 - categorical_accuracy: 0.9570 - val_loss: 1.6280 - val_categorical_accuracy: 0.5000 - lr: 8.0000e-06\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1368 - categorical_accuracy: 0.9646\n",
      "Epoch 16: saving model to model_init_2024-02-2710_01_40.642789\\model-00016-0.13684-0.96456-1.05805-0.68000.h5\n",
      "34/34 [==============================] - 201s 6s/step - loss: 0.1368 - categorical_accuracy: 0.9646 - val_loss: 1.0581 - val_categorical_accuracy: 0.6800 - lr: 8.0000e-06\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1485 - categorical_accuracy: 0.9608\n",
      "Epoch 17: saving model to model_init_2024-02-2710_01_40.642789\\model-00017-0.14851-0.96078-0.81400-0.78000.h5\n",
      "34/34 [==============================] - 204s 6s/step - loss: 0.1485 - categorical_accuracy: 0.9608 - val_loss: 0.8140 - val_categorical_accuracy: 0.7800 - lr: 8.0000e-06\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1345 - categorical_accuracy: 0.9570\n",
      "Epoch 18: saving model to model_init_2024-02-2710_01_40.642789\\model-00018-0.13454-0.95701-0.75911-0.73000.h5\n",
      "34/34 [==============================] - 203s 6s/step - loss: 0.1345 - categorical_accuracy: 0.9570 - val_loss: 0.7591 - val_categorical_accuracy: 0.7300 - lr: 8.0000e-06\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1339 - categorical_accuracy: 0.9691\n",
      "Epoch 19: saving model to model_init_2024-02-2710_01_40.642789\\model-00019-0.13387-0.96908-0.40487-0.86000.h5\n",
      "34/34 [==============================] - 205s 6s/step - loss: 0.1339 - categorical_accuracy: 0.9691 - val_loss: 0.4049 - val_categorical_accuracy: 0.8600 - lr: 8.0000e-06\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1356 - categorical_accuracy: 0.9600\n",
      "Epoch 20: saving model to model_init_2024-02-2710_01_40.642789\\model-00020-0.13557-0.96003-0.46406-0.84000.h5\n",
      "34/34 [==============================] - 203s 6s/step - loss: 0.1356 - categorical_accuracy: 0.9600 - val_loss: 0.4641 - val_categorical_accuracy: 0.8400 - lr: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn1_model.count_params())\n",
    "history_model9=rnn_cnn1.train_model(rnn_cnn1_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
